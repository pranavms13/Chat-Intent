{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Intent Identification Testing\n",
    "Date : 01-09-2019\n",
    "#### Authors :\n",
    "Pranav M.S   \n",
    ". . Add Names Here . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'tagger': 36.476383209228516}\n",
      "Losses {'tagger': 36.09692859649658}\n",
      "Losses {'tagger': 34.84562110900879}\n",
      "Losses {'tagger': 31.881956100463867}\n",
      "Losses {'tagger': 27.57524585723877}\n",
      "Losses {'tagger': 23.918304443359375}\n",
      "Losses {'tagger': 19.23468589782715}\n",
      "Losses {'tagger': 12.826183319091797}\n",
      "Losses {'tagger': 7.783919334411621}\n",
      "Losses {'tagger': 3.8060667514801025}\n",
      "Losses {'tagger': 1.4551175236701965}\n",
      "Losses {'tagger': 0.499441534280777}\n",
      "Losses {'tagger': 0.15054754633456469}\n",
      "Losses {'tagger': 0.02907222742214799}\n",
      "Losses {'tagger': 0.007075784495100379}\n",
      "Losses {'tagger': 0.0016412516706623137}\n",
      "Losses {'tagger': 0.0005066609592176974}\n",
      "Losses {'tagger': 0.00010562757233856246}\n",
      "Losses {'tagger': 2.7612188205239363e-05}\n",
      "Losses {'tagger': 8.94600452738814e-06}\n",
      "Losses {'tagger': 3.669594775601581e-06}\n",
      "Losses {'tagger': 1.6928210868627502e-06}\n",
      "Losses {'tagger': 7.240153223619927e-07}\n",
      "Losses {'tagger': 4.751405953129506e-07}\n",
      "Losses {'tagger': 2.8665143503303625e-07}\n",
      "Tags [('I', 'NOUN', 'NOUN'), ('am', 'VERB', 'VERB'), ('sad', 'ADJ', 'ADJ'), (',', 'ADJ', 'ADJ'), ('angry', 'ADJ', 'ADJ'), ('and', 'ADJ', 'ADJ'), ('depressed', 'ADJ', 'ADJ')]\n"
     ]
    }
   ],
   "source": [
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "TAG_MAP = {\n",
    "\"NOUN\": {\"pos\": \"NOUN\"}, #noun\n",
    "\"VERB\": {\"pos\": \"VERB\"}, #verb\n",
    "\"ADJ\": {\"pos\": \"ADJ\"}, #adjective\n",
    "\"ADP\": {\"pos\": \"ADP\"}, #adposition\n",
    "\"ADV\": {\"pos\": \"ADV\"}, #adverb\n",
    "\"AUX\": {\"pos\": \"AUX\"}, #auxiliary verb\n",
    "\"CONJ\": {\"pos\": \"CONJ\"}, #coordinating conjunction\n",
    "\"DET\": {\"pos\": \"DET\"}, #determiner\n",
    "\"INTJ\": {\"pos\": \"INTJ\"}, #interjection\n",
    "\"NUM\": {\"pos\": \"NUM\"}, #numeral\n",
    "\"PART\": {\"pos\": \"PART\"}, #particle\n",
    "\"PRON\": {\"pos\": \"PRON\"}, #pronoun\n",
    "\"PROPN\": {\"pos\": \"PROPN\"}, #proper noun\n",
    "\"PUNCT\": {\"pos\": \"PUNCT\"}, #punctuation\n",
    "\"SCONJ\": {\"pos\": \"SCONJ\"}, #subordinating conjunction\n",
    "\"SYM\": {\"pos\": \"SYM\"} #symbol\n",
    "}\n",
    "\n",
    "TRAIN_DATA = [\n",
    "    (\"Hi hello hola namaste ola welcome .\",{\"tags\": [\"INTJ\", \"INTJ\", \"INTJ\", \"INTJ\", \"INTJ\", \"INTJ\",\"SYM\"]}),\n",
    "    (\"I like green eggs\", {\"tags\": [\"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"]}),\n",
    "    (\"Eat blue ham\", {\"tags\": [\"VERB\", \"ADJ\", \"NOUN\"]}),\n",
    "    (\"Cat sat on a Mat\",{\"tags\": [\"NOUN\",\"VERB\",\"ADV\",\"DET\",\"NOUN\"]}),\n",
    "    (\"A treat for fans of Bob Marley\",{\"tags\":[\"DET\",\"VERB\",\"ADP\",\"NOUN\",\"ADP\",\"NOUN\",\"NOUN\"]}),\n",
    "    (\"How are you ?\",{\"tags\":[\"ADV\",\"VERB\",\"PRON\",\"SYM\"]}),\n",
    "    (\"I am angry sad depressed lonely ,\", {\"tags\": [\"NOUN\", \"VERB\", \"ADJ\", \"ADJ\", \"ADJ\", \"ADJ\", \"SYM\"]})\n",
    "]\n",
    "\n",
    "#-------------------\n",
    "lang = \"en\"\n",
    "output_dir = None\n",
    "n_iter = 25\n",
    "#-------------------\n",
    "\n",
    "nlp = spacy.blank(lang)\n",
    "\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "for tag, values in TAG_MAP.items():\n",
    "    tagger.add_label(tag, values)\n",
    "nlp.add_pipe(tagger)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(n_iter):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    \n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "    print(\"Losses\", losses)\n",
    "\n",
    "test_text = \"I am sad, angry and depressed\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])\n",
    "\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n",
    "\n",
    "    print(\"Loading from\", output_dir)\n",
    "    nlp2 = spacy.load(output_dir)\n",
    "    doc = nlp2(test_text)\n",
    "    print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
